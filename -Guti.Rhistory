# Print the skew rounded to 2 decimal places
print(round(skew, 2))
# Calculate the skewness and standard error of skew
ses <- round(sqrt(6/length(df_strenth$Density)),2)
ses
kurtosis <- round(kurtosis(df_strenth$Density, type = 1) - 3, 2)
kurtosis
# Calculate standard error of kurtosis (SES)
ses_kurt <- round(sqrt(24/length(df_strenth$Density)),2)
ses_kurt
shapiro.test(df_strenth$Density)
hist(df_strenth$Density)
hist(df_strenth$Density, breaks = 6)
hist(df_strenth$Density, breaks = 10)
df_strenth <- read.csv('Boardstrength.csv')
setwd("C:/Users/pgutierr/OneDrive - University of Vermont/Curriculum/07_ Cursos/Stats_Undergrad/Problem set_ M4 Descriptive Statistic")
df_strenth <- read.csv('Boardstrength.csv')
summary(df_strenth)
df_strenth <- read.csv('Boardstrength.csv')
summary(df_strenth)
**Load the water pollution data into R.<br />**
1. the **MEAN Strength**.
1.1. the **MEDIAN Strength**.
round(median(df_strenth$Strength),2)
round(mean(df_strenth$Strength),2)
round(sd(df_strenth$Strength),2)
round(var(df_strenth$Strength),2)
# Calculate the IQR for the specified column
round(iqr_value <- IQR(df_strenth$Strength),2)
# Calculate the quartiles and IQR
q1 <- quantile(df_strenth$Strength, 0.25, na.rm = TRUE)
q3 <- quantile(df_strenth$Strength, 0.75, na.rm = TRUE)
iqr <- q3 - q1
# Define the lower and upper bounds for outliers
lower_bound <- q1 - 1.5 * iqr
upper_bound <- q3 + 1.5 * iqr
# Identify outlier years
outlier_years <- df_strenth$Strength < lower_bound | df_strenth$Strength > upper_bound
# Count the number of outlier years
num_outliers <- sum(outlier_years)
# Print the number of outlier years
print(num_outliers)
# Calculate Pearson's skew
skew <- skew(df_strenth$Density)
library(tidyverse)
library(lessR)
library(DT)
library(e1071)
# Calculate Pearson's skew
skew <- skew(df_strenth$Density)
# Print the skew rounded to 2 decimal places
print(round(skew, 2))
# Calculate the skewness and standard error of skew
ses <- round(sqrt(6/length(df_strenth$Density)),2)
ses
kurtosis <- round(kurtosis(df_strenth$Density, type = 1) - 3, 2)
kurtosis
# Calculate standard error of kurtosis (SES)
ses_kurt <- round(sqrt(24/length(df_strenth$Density)),2)
ses_kurt
shapiro.test(df_strenth$Density)
hist(df_strenth$Density, breaks = 10)
library(tidyverse)
library(lessR)
library(DT)
library(e1071)
df_strenth <- read.csv('Boardstrength.csv')
summary(df_strenth)
setwd("C:/Users/pgutierr/OneDrive - University of Vermont/Curriculum/07_ Cursos/Stats_Undergrad/Problem set_ M4 Descriptive Statistic")
df_strenth <- read.csv('Boardstrength.csv')
summary(df_strenth)
# Calculate Pearson's skew
skew <- skew(df_strenth$Density)
# Print the skew rounded to 2 decimal places
print(round(skew, 2))
# Calculate the skewness and standard error of skew
ses <- round(sqrt(6/length(df_strenth$Density)),2)
ses
kurtosis <- round(kurtosis(df_strenth$Density, type = 1) - 3, 2)
kurtosis
# Calculate standard error of kurtosis (SES)
ses_kurt <- round(sqrt(24/length(df_strenth$Density)),2)
ses_kurt
shapiro.test(df_strenth$Density)
setwd("C:/Users/pgutierr/OneDrive - University of Vermont/Curriculum/07_ Cursos/Stats_Undergrad/Problem set_ M4 Descriptive Statistic")
library(tidyverse)
library(lessR)
library(DT)
library(e1071)
df_strenth <- read.csv('Boardstrength.csv')
summary(df_strenth)
df_strenth <- read.csv('Boardstrength.csv')
summary(df_strenth)
**Load the water pollution data into R.<br />**
1. the **MEAN Strength**.
1.1. the **MEDIAN Strength**.
2. Now calculate the **STANDARD DEVIATION** for the Strength.
2.1
```{r}
3. What is the **inter-quartile range** for Strength column?
# Print the IQR
print(iqr_value)
# Print the IQR
print(iqr_value)
# Print the IQR
print(iqr_value)
4. Using the Interquartile range technique, how many **OUTLIER** years are there in your
# Define the lower and upper bounds for outliers
lower_bound <- q1 - 1.5 * iqr
upper_bound <- q3 + 1.5 * iqr
# Identify outlier years
outlier_years <- df_strenth$Strength < lower_bound | df_strenth$Strength > upper_bound
# Count the number of outlier years
num_outliers <- sum(outlier_years)
# Print the number of outlier years
print(num_outliers)
# Print the number of outlier years
print(num_outliers)
# Print the number of outlier years
print(num_outliers)
5. Now, for the **Density** column, calculate the **skewness, standard error of skewness (SES), kurtosis, standard error of kurtosis (SEK)**, and perform the **normality test with Shapiro-Wilk test**. Based on all of these parameters, determine if the Density is normally distributed.
5.1. Calculate the **standard error of skew (ses)** for this data so we can determine the
5.2. Based on the ses technique, is this **skew significant**?
kurtosis <- round(kurtosis(df_strenth$Density, type = 1) - 3, 2)
kurtosis
kurtosis <- round(kurtosis(df_strenth$Density, type = 1), 2)
kurtosis
kurtosis <- round(kurtosis(df_strenth$Density, type = 1) - 3, 2)
kurtosis
# Calculate standard error of kurtosis (SES)
ses_kurt <- round(sqrt(24/length(df_strenth$Density)),2)
ses_kurt
5.6. Based on the sek technique, is this **kurtosis significant**?
5.7. Based on all of these parameters, determine if the Yearly Mean Snow Depth is normally distributed. Pay special attention to the shape (distribution) of the data (plotted on a histogram).
5.8. Is your data normally distributed? How can you tell? Be sure to report your certainty of this conclusion.
```{r}
shapiro.test(df_strenth$Density)
hist(df_strenth$Density, breaks = 10)
kurtosis(df_strenth$Density)
kurtosis(df_strenth$Density)-3
kurtosis <- round(kurtosis(df_strenth$Density) - 3, 2)
kurtosis
kurtosis <- round(kurtosis(df_strenth$Density, type = 1) - 3, 2)
kurtosis
pivot(df_strenth, c(IQR, skew, kurtosis, mean, sd, var), Density)
df_water <- read.csv('C:/Users/Guti/OneDrive - University of Vermont/Curriculum/07_ Cursos/NR-2400/Problem set_ M6 One Sample Z-test/water pollution.csv')
df_water <- read.csv('water pollution.csv')
setwd("C:/Users/pgutierr/OneDrive - University of Vermont/Curriculum/07_ Cursos/Stats_Undergrad/Problem set_ M6 One Sample Z-test")
df_water <- read.csv('water pollution.csv')
head(df_water)
head(df_water,10)
z.test(x= df_water,
alternative = "greater",
mu = 1644,
sigma.x = 497)
library(BSDA)
df_water <- read.csv('water pollution.csv')
head(df_water,10)
df_water <- read.csv('water pollution.csv')
head(df_water,10)
**Load the water pollution data into R.<br />**
```{r}
z.test(x= df_water,
alternative = "greater",
mu = 1644,
sigma.x = 497)
head(df_water,10)
z.test(x= df_water$value,
alternative = "greater",
mu = 1644,
sigma.x = 497)
z.test(x= df_water$value,
alternative = "greater",
mu = 1644,
sigma.x = 497)
# Given data
population_mean <- 85
population_sd <- 11.6
sample_mean <- 80.94
sample_size <- 25
# Perform one-sample z-test
result <- z.test(x = sample_mean, sigma.x = population_sd, n = sample_size, mu = population_mean, alternative = "less")
# Given data
population_mean <- 85
population_sd <- 11.6
sample_mean <- 80.94
n <- 25
# Perform one-sample z-test
# Calculate the Z-test statistic
z <- (sample_mean - population_mean) / (population_sd / sqrt(n))
z
# Two-tailed test, so multiply p-value by 2
p_value <- 2 * pnorm(abs(z), lower.tail = T)
p_value
# Two-tailed test, so multiply p-value by 2
p_value <- pnorm(abs(z), lower.tail = T)
p_value
# Two-tailed test, so multiply p-value by 2
p_value <- pnorm(abs(z), lower.tail = TRUE) * 2
p_value
# Two-tailed test, so multiply p-value by 2
p_value <- pnorm(abs(z), lower.tail = TRUE)
p_value
# Two-tailed test, so multiply p-value by 2
p_value <- pnorm(abs(z), lower.tail = F)
p_value
# Two-tailed test, so multiply p-value by 2
p_value <- pnorm(abs(z))
p_value
# Two-tailed test, so multiply p-value by 2
p_value <- pnorm(abs(z), lower.tail = T)
p_value
# Two-tailed test, so multiply p-value by 2
p_value <- pnorm(abs(z), lower.tail = F)
p_value
# Given data
sample_mean <- 75
population_mean <- 60
population_sd <- 30
n <- 36
# Calculate Z-test statistic
z <- (sample_mean - population_mean) / (population_sd / sqrt(n))
# When lower.tail = FALSE, it calculates the probability that a random variable from the
# distribution is greater than the given value.
p_value <- pnorm(abs(z), lower.tail = FALSE)
p_value
# Two-tailed test, so multiply p-value by 2
p_value <- pnorm(abs(z), lower.tail = T)
p_value
# Two-tailed test, so multiply p-value by 2
p_value <- 1-(pnorm(abs(z), lower.tail = T))
p_value
# Given data
population_mean <- 85
population_sd <- 11.6
sample_mean <- 80.94
n <- 25
# Perform one-sample z-test
# Calculate the Z-test statistic
z <- (sample_mean - population_mean) / (population_sd / sqrt(n))
z
# Two-tailed test, so multiply p-value by 2
p_value <- 1-(pnorm(abs(z), lower.tail = T))
p_value
# Two-tailed test, so multiply p-value by 2
p_value <- 1-(pnorm(abs(z), lower.tail = F))
p_value
# Two-tailed test, so multiply p-value by 2
p_value <- pnorm(abs(z), lower.tail = F)
p_value
# Given data
population_mean <- 85
population_sd <- 11.6
sample_mean <- 80.94
n <- 25
# Given data
population_mean <- 85
population_sd <- 11.6
sample_mean <- 80.94
n <- 25
# Perform one-sample z-test
# Calculate the Z-test statistic
z <- (sample_mean - population_mean) / (population_sd / sqrt(n))
z
# Two-tailed test, so multiply p-value by 2
p_value <- pnorm(abs(z), lower.tail = F)
p_value
df_water <- read.csv('water pollution.csv')
setwd("C:/Users/pgutierr/OneDrive - University of Vermont/Curriculum/07_ Cursos/Stats_Undergrad/Problem set_ M6 One Sample Z-test/Problem Set")
df_water <- read.csv('water pollution.csv')
head(df_water,10)
df_water <- read.csv('water pollution.csv')
head(df_water,10)
**Load the water pollution data into R.<br />**
```{r}
z.test(x= df_water$value,
alternative = "greater",
mu = 1644,
sigma.x = 497)
library(BSDA)
z.test(x= df_water$value,
alternative = "greater",
mu = 1644,
sigma.x = 497)
# Two-tailed test, so multiply p-value by 2
p_value <- pnorm(z, lower.tail = T)
# Given data
population_mean <- 85
population_sd <- 11.6
sample_mean <- 80.94
n <- 25
# Perform one-sample z-test
# Calculate the Z-test statistic
z <- (sample_mean - population_mean) / (population_sd / sqrt(n))
z
# Two-tailed test, so multiply p-value by 2
p_value <- pnorm(z, lower.tail = T)
p_value
df_water <- read.csv('water pollution.csv')
head(df_water,10)
# Perform one-sample z-test
# Calculate the Z-test statistic
z <- (sample_mean - population_mean) / (population_sd / sqrt(n))
# Given data
population_mean <- 85
population_sd <- 11.6
sample_mean <- 80.94
n <- 25
# Perform one-sample z-test
# Calculate the Z-test statistic
z <- (sample_mean - population_mean) / (population_sd / sqrt(n))
z
# Two-tailed test, so multiply p-value by 2
p_value <- pnorm(z, lower.tail = T)
p_value
df_water <- read.csv('water pollution.csv')
setwd("C:/Users/pgutierr/OneDrive - University of Vermont/Curriculum/07_ Cursos/Stats_Undergrad/Problem set_ M6 One Sample Z-test/Problem Set")
df_water <- read.csv('water pollution.csv')
head(df_water,10)
crop.data <- read.csv(file = "crop.data.csv", header = TRUE)
crop.data <- read.csv(file = "crop.data.csv", header = TRUE)
setwd("C:/Users/pgutierr/OneDrive - University of Vermont/Curriculum/07_ Cursos/Stats_Undergrad/Module 8 Anova/Problem Set")
crop.data <- read.csv(file = "crop.data.csv", header = TRUE)
crop.data <- read.csv(file = "crop.data.csv", header = TRUE)
#3. Run a Simple Linear Regresion
```{r}
one.way <- aov(yield ~ fertilizer, data = crop.data)
summary(one.way)
crop.data <- read.csv(file = "crop.data.csv", header = TRUE)
crop.data <- read.csv(file = "crop.data.csv", header = TRUE)
#3. Run a Simple Linear Regresion
```{r}
one.way <- aov(yield ~ fertilizer, data = crop.data)
summary(one.way)
#normality
by(crop.data$yield, crop.data$fertilizer, shapiro.test)
#variance
bartlett.test(yield ~ fertilizer, data=crop.data)
TukeyHSD(one.way)
plot(TukeyHSD(one.way))
library(stats4nr)
install.packages('stats4nr)
install.packages('stats4nr')
install.packages('stats4nr')
library(stats4nr)
library(stats4nr)
install.packages('stats4nr')
library(stats4nr)
install.packages("stats4nr")
iron
data(iron)
crop.data <- read.csv(file = "crop.data.csv", header = TRUE)
data(iron)
crop.data <- read.csv(file = "crop.data.csv", header = TRUE)
crop.data
crop.data <- read.csv(file = "crop.data.csv", header = TRUE)
crop.data <- read.csv(file = "crop.data.csv", header = TRUE)
#.Normality
```{r}
#Variance
```{r}
#3. Run a Simple Linear Regresion
```{r}
#4. Perform a Tukey’s Honestly Significant Difference (Tukey’s HSD) post-hoc test for pairwise comparisons
```{r}
one.way <- aov(yield ~ fertilizer, data = crop.data)
summary(one.way)
TukeyHSD(one.way)
#normality
by(crop.data$yield, crop.data$fertilizer, shapiro.test)
#variance
bartlett.test(yield ~ fertilizer, data=crop.data)
one.way <- aov(yield ~ fertilizer, data = crop.data)
summary(one.way)
library(PMCMRplus)
library(palmerpenguins)
library(tidyverse)
library(lessR)
library(readxl)
library(RVAideMemoire)
library(car)
library(DescTools)
library(rstatix)
library(PMCMRplus)
library(palmerpenguins)
# Compute the analysis of variance
# This just does it, but doesn't show it to you!
penguin_aov <- aov(flipper_length_mm ~ species, data = penguins)
# Here is how we look at it: Summary of the analysis
# What is the p-value? Look at "Pr(>F)"
summary(penguin_aov)
#normality
by(crop.data$yield, crop.data$fertilizer, shapiro.test)
#variance
bartlett.test(yield ~ fertilizer, data=crop.data)
TukeyHSD(one.way)
plot(TukeyHSD(one.way))
library(tidyverse)
library(lessR)
library(readxl)
library(RVAideMemoire)
library(car)
library(DescTools)
library(rstatix)
library(PMCMRplus)
library(palmerpenguins)
https://statsandr.com/blog/anova-in-r/
https://statsandr.com/blog/anova-in-r/
https://statsandr.com/blog/anova-in-r/
penguins
#But we don't know WHICH species are different from each other: here is that
pairwise.t.test(penguins$flipper_length_mm, penguins$species,
p.adjust.method = "bonferroni")
#But we don't know WHICH species are different from each other: here is that
pairwise.t.test(yield ~ fertilizer, data = crop.data,
p.adjust.method = "bonferroni")
#But we don't know WHICH species are different from each other: here is that
pairwise.t.test(yield,  fertilizer, data = crop.data,
p.adjust.method = "bonferroni")
#But we don't know WHICH species are different from each other: here is that
pairwise.t.test(crop.data$yield,  crop.data$fertilizer,
p.adjust.method = "bonferroni")
plot(TukeyHSD(one.way))
TukeyHSD(one.way)
library(tidyverse)
library(lessR)
library(readxl)
library(dplyr)
UV_deformities <- read.csv(file = "UV_deformities.csv", header = TRUE)
setwd("C:/Users/Guti/OneDrive - University of Vermont/Curriculum/07_ Cursos/Stats_Undergrad/Module 10 Anova/Group Work")
UV_deformities <- read.csv(file = "UV_deformities.csv", header = TRUE)
UV_deformities
shapiro.test(UV_deformities$deformities)
one.way <- aov(deformities ~ species, data = UV_deformities)
summary(one.way)
wilcox_test(deformities ~ species, data = UV_deformities, p.adjust.method = "bonferroni")
library(rstatix)
library(rstatix)
wilcox_test(deformities ~ species, data = UV_deformities, p.adjust.method = "bonferroni")
kruskal_effsize(deformities ~ species, data = UV_deformities)
kruskal.test(deformities ~ species, data = UV_deformities)
summary(one.way)
(18047+9460)/9460
(18047)/18047+9460
(18047)/(18047+9460)
# Perform Steel-Dwass all-pairs comparison
pairwise_result <- posthoc.kruskal.dunn.test(UV_deformities$deformities, UV_deformities$species, p.adjust.method = "bonferroni")
library(PMCMRplus)
# Perform Steel-Dwass all-pairs comparison
pairwise_result <- posthoc.kruskal.dunn.test(UV_deformities$deformities, UV_deformities$species, p.adjust.method = "bonferroni")
library(tidyverse)
library(rstatix)
library(readxl)
library(dplyr)
library(PMCMRplus)
# Perform Steel-Dwass all-pairs comparison
pairwise_result <- posthoc.kruskal.dunn.test(UV_deformities$deformities, UV_deformities$species, p.adjust.method = "bonferroni")
# Perform Steel-Dwass all-pairs comparison
# Perform Steel-Dwass all-pairs comparison
pairwise_result <- posthoc.kruskal.nemenyi.test(UV_deformities$deformities, UV_deformities$species, p.adj = "bonferroni")
# Perform Steel-Dwass all-pairs comparison
dscfAllPairsTest(deformities ~ species, data = UV_deformities)
kruskal_effsize(flipper_length_mm ~ species, data = penguins)
kruskal_effsize(deformities ~ species, data = UV_deformities)
wilcox_test(deformities ~ species, data = UV_deformities, p.adjust.method = "bonferroni")
wilcox_test(deformities ~ species, data = UV_deformities)
wilcox_result <- wilcox.test(deformities ~ species, data = UV_deformities)
wilcox_test(deformities ~ species, data = UV_deformities)
wilcox.test(deformities ~ species, data = UV_deformities)
wilcox_result <- wilcox_test(deformities ~ species, data = UV_deformities)
# Check the Wilcoxon test results
print(wilcox_result)
# Check the Wilcoxon test results
summary(wilcox_result)
one.way <- aov(deformities ~ species, data = UV_deformities)
summary(one.way)
(18047)/(18047+9460)
welch_anova_test(deformities ~ species, data = UV_deformities)
kruskal_test(deformities ~ species, data = UV_deformities)
wilcox_test(deformities ~ species, data = UV_deformities, p.adjust.method = "bonferroni")
one.way <- aov(deformities ~ species, data = UV_deformities)
summary(one.way)
kruskal.test(deformities ~ species, data = UV_deformities)
kruskal_test(deformities ~ species, data = UV_deformities)
kruskal.test(deformities ~ species, data = UV_deformities)
#All Pairs
wilcox_test(deformities ~ species, data = UV_deformities, p.adjust.method = "bonferroni")
#Effect Size
kruskal_effsize(deformities ~ species, data = UV_deformities)
ggplot(UV_deformities) + aes(x = species, fill = deformities) + geom_boxplot()
ggplot(UV_deformities) + aes(x = species, y = deformities) + geom_boxplot()
ggplot(UV_deformities) + aes(x = species, y = deformities) +
geom_boxplot()
bartlett.test(deformities ~ species, data = UV_deformities)
setwd("C:/Users/Guti/OneDrive - University of Vermont/Curriculum/07_ Cursos/Stats_Undergrad/Module 10 Anova/Group Work")
library(tidyverse)
library(rstatix)
library(readxl)
library(dplyr)
library(PMCMRplus)
library(tidyverse)
library(rstatix)
library(readxl)
library(dplyr)
library(PMCMRplus)
UV_deformities <- read.csv(file = "UV_deformities.csv", header = TRUE)
UV_deformities
shapiro.test(UV_deformities$deformities)
bartlett.test(deformities ~ species, data = UV_deformities)
kruskal.test(deformities ~ species, data = UV_deformities)
#All Pairs
wilcox_test(deformities ~ species, data = UV_deformities, p.adjust.method = "bonferroni")
